<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Yin's Blog</title><link href="https://IMYin.github.io/" rel="alternate"></link><link href="https://IMYin.github.io/feeds/ji-zhu-wen-zhang.atom.xml" rel="self"></link><id>https://IMYin.github.io/</id><updated>2016-09-28T21:25:00+08:00</updated><entry><title>决策树－非线性分类与回归</title><link href="https://IMYin.github.io/2016/09/28/%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%8D%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E4%B8%8E%E5%9B%9E%E5%BD%92.html" rel="alternate"></link><published>2016-09-28T21:25:00+08:00</published><updated>2016-09-28T21:25:00+08:00</updated><author><name>Yin</name></author><id>tag:IMYin.github.io,2016-09-28:2016/09/28/决策树－非线性分类与回归.html</id><summary type="html">&lt;h3&gt;决策树简介&lt;/h3&gt;
&lt;p&gt;决策树就像一个树状的决策模型。它通常将解释变量递归地切分成子集来学习，如下图所示。决策树的节点用方块表示，用来测试解释变量。&lt;/p&gt;
&lt;p&gt;这些节点通过边缘连接来指定输出。比如用某个节点来测试解释变量是否超出了阈值，如果没有超过，就指向左边的节点，如果超过了，就指向右边节点。就这样重复这一过程，到达终止条件即停止。在分类问题中，叶子节点就代表着类别；在回归问题中，所有响应变量的值取平均值来作为响应变量的估计值。　　
&lt;center&gt;&lt;img alt="1" src="https://IMYin.github.io/2016/09/28/icons/em_tree.jpg" /&gt;
&lt;/center&gt;  &lt;/p&gt;
&lt;h3&gt;训练决策树&lt;/h3&gt;
&lt;p&gt;我们开始先用&lt;a href="https://en.wikipedia.org/wiki/ID3_algorithm"&gt;ID３(Iterative Dichotomiser 3)&lt;/a&gt;算法来构建决策树。&lt;br /&gt;
假如现在我们需要把一些猫和狗进行分类。但是你不可以通过肉眼观察，而是通过一些它们的行为特征来分类。这些变量也就是所说的解释变量，比如是否喜欢打闹，脾气是否暴躁，三类食物中那类是最喜欢的。
为了对其分类，决策树需要把解释变量作为树节点来测试。这些节点的下一节点在于它们这次的测试结果是什么。直到最到抵达叶子节点，也就测试出是猫是狗。　　
&lt;center&gt;&lt;img alt="2" src="https://IMYin.github.io/2016/09/28/icons/train_table_dog_cat.jpg" /&gt;
&lt;/center&gt;&lt;br /&gt;
观察这些数据可以看到，猫比狗更容易发脾气。大多数狗玩球，而猫不爱玩。狗更喜欢狗粮和培根，而猫喜欢猫粮和培根。解释变量是否喜欢玩球和是否经常发脾气可以很简单得转换成二元特征值，而喜欢的食物有三个值，我们将用&lt;a href="https://en.wikipedia.org/wiki/One-hot"&gt;独热编码(One-hot)&lt;/a&gt;来表示([1,0,0]，[0,1,0]，[0,0,1])。&lt;/p&gt;</summary><category term="Machine Learning"></category></entry></feed>