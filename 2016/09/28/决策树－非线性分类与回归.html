<!DOCTYPE html>
<html lang="zh" prefix="og: http://ogp.me/ns# fb: https://www.facebook.com/2008/fbml">
<head>
    <title>决策树－非线性分类与回归 - Yin's Blog</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">



<link rel="canonical" href="https://IMYin.github.io/2016/09/28/决策树－非线性分类与回归.html">

        <meta name="author" content="Yin" />
        <meta name="keywords" content="Machine Learning" />
        <meta name="description" content="决策树简介 决策树就像一个树状的决策模型。它通常将解释变量递归地切分成子集来学习，如下图所示。决策树的节点用方块表示，用来测试解释变量。 这些节点通过边缘连接来指定输出。比如用某个节点来测试解释变量是否超出了阈值，如果没有超过，就指向左边的节点，如果超过了，就指向右边节点。就这样重复这一过程，到达终止条件即停止。在分类问题中，叶子节点就代表着类别；在回归问题中，所有响应变量的值取平均值来作为响应变量的估计值。 训练决策树 我们开始先用 ..." />

        <meta property="og:site_name" content="Yin's Blog" />
        <meta property="og:type" content="article"/>
        <meta property="og:title" content="决策树－非线性分类与回归"/>
        <meta property="og:url" content="https://IMYin.github.io/2016/09/28/决策树－非线性分类与回归.html"/>
        <meta property="og:description" content="决策树简介 决策树就像一个树状的决策模型。它通常将解释变量递归地切分成子集来学习，如下图所示。决策树的节点用方块表示，用来测试解释变量。 这些节点通过边缘连接来指定输出。比如用某个节点来测试解释变量是否超出了阈值，如果没有超过，就指向左边的节点，如果超过了，就指向右边节点。就这样重复这一过程，到达终止条件即停止。在分类问题中，叶子节点就代表着类别；在回归问题中，所有响应变量的值取平均值来作为响应变量的估计值。 训练决策树 我们开始先用 ..."/>
        <meta property="article:published_time" content="2016-09-28" />
            <meta property="article:section" content="技术文章" />
            <meta property="article:tag" content="Machine Learning" />
            <meta property="article:author" content="Yin" />


    <!-- Bootstrap -->
        <link rel="stylesheet" href="https://IMYin.github.io/theme/css/bootstrap.min.css" type="text/css"/>
    <link href="https://IMYin.github.io/theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="https://IMYin.github.io/theme/css/pygments/native.css" rel="stylesheet">
    <link rel="stylesheet" href="https://IMYin.github.io/theme/css/style.css" type="text/css"/>

        <link href="https://IMYin.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate"
              title="Yin's Blog ATOM Feed"/>



        <link href="https://IMYin.github.io/feeds/ji-zhu-wen-zhang.atom.xml" type="application/atom+xml" rel="alternate"
              title="Yin's Blog 技术文章 ATOM Feed"/>

</head>
<body>

<div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
	<div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="https://IMYin.github.io/" class="navbar-brand">
Yin's Blog            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
                        <li class="active">
                            <a href="https://IMYin.github.io/category/ji-zhu-wen-zhang.html">技术文章</a>
                        </li>
            </ul>
            <ul class="nav navbar-nav navbar-right">
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->
<!-- Banner -->
<!-- End Banner -->
<div class="container">
    <div class="row">
        <div class="col-lg-12">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="https://IMYin.github.io/2016/09/28/决策树－非线性分类与回归.html"
                       rel="bookmark"
                       title="Permalink to 决策树－非线性分类与回归">
                        决策树－非线性分类与回归
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="label label-default">Date</span>
    <span class="published">
        <i class="fa fa-calendar"></i><time datetime="2016-09-28T21:25:00+08:00"> 2016-09-28(Wed)</time>
    </span>





<span class="label label-default">Tags</span>
	<a href="https://IMYin.github.io/tag/machine-learning.html">Machine Learning</a>
    
</footer><!-- /.post-info -->                    </div>
                </div>
                <h3>决策树简介</h3>
<p>决策树就像一个树状的决策模型。它通常将解释变量递归地切分成子集来学习，如下图所示。决策树的节点用方块表示，用来测试解释变量。</p>
<p>这些节点通过边缘连接来指定输出。比如用某个节点来测试解释变量是否超出了阈值，如果没有超过，就指向左边的节点，如果超过了，就指向右边节点。就这样重复这一过程，到达终止条件即停止。在分类问题中，叶子节点就代表着类别；在回归问题中，所有响应变量的值取平均值来作为响应变量的估计值。　　
<center><img alt="1" src="https://IMYin.github.io/2016/09/28/icons/em_tree.jpg" />
</center>  </p>
<h3>训练决策树</h3>
<p>我们开始先用<a href="https://en.wikipedia.org/wiki/ID3_algorithm">ID３(Iterative Dichotomiser 3)</a>算法来构建决策树。<br />
假如现在我们需要把一些猫和狗进行分类。但是你不可以通过肉眼观察，而是通过一些它们的行为特征来分类。这些变量也就是所说的解释变量，比如是否喜欢打闹，脾气是否暴躁，三类食物中那类是最喜欢的。</p>
<p>为了对其分类，决策树需要把解释变量作为树节点来测试。这些节点的下一节点在于它们这次的测试结果是什么。直到最到抵达叶子节点，也就测试出是猫是狗。　　
<center><img alt="2" src="https://IMYin.github.io/2016/09/28/icons/train_table_dog_cat.jpg" />
</center><br />
观察这些数据可以看到，猫比狗更容易发脾气。大多数狗玩球，而猫不爱玩。狗更喜欢狗粮和培根，而猫喜欢猫粮和培根。解释变量是否喜欢玩球和是否经常发脾气可以很简单得转换成二元特征值，而喜欢的食物有三个值，我们将用<a href="https://en.wikipedia.org/wiki/One-hot">独热编码(One-hot)</a>来表示([1,0,0]，[0,1,0]，[0,0,1])。</p>
<p>到此，我们可以手动得去构建一个分类规则，比如说喜欢打闹并且喜欢吃培根那就是只狗，以此类推。但是手动构建这些规则比较麻烦，我们可以通过构建决策树来制定规则。</p>
<h3>问题选择</h3>
<p>在决策树中，我们通常先得对解释变量进行测试，然后通过测试得到的值去预测出响应变量。但是哪些解释变量放在前面会产生更好的输出，以形成好的model？我们的思路是通过测试得到的子集中包含所有的猫或者所有的狗，这样比得到的子集中既有猫又有狗要好很多。我们还需要避免创建那种测试，把单独的一只猫或一条狗分离出去。换句话说，也就是通过测试要最大程度的降低我们的不确定性。这里就涉及到一个用来度量信息不确定性的概念：<a href="https://zh.wikipedia.org/wiki/%E7%86%B5">熵(entropy)</a>。</p>
<p>熵的单位是$bit$，它量化了一个变量的不确定性。它的方程表示如下，其中$n$代表会有多少个输出，$P(x_i)$代表第$i$个输出的概率。$b$一般取$２$，$e$一般取$10$。由于$P$会小于$1$，那么取得对数为负数，所以前面加个负号让其变为正。$$H(X) = -\sum_{i=1}^nP(x_i)log_bP(x_i)$$</p>
<p>比如，投硬币正面朝上的概率是$0.5$,正面朝下的概率是$０。５$,那么这个硬币投一次所得到的熵就等于：$$H(X) = -(0.5log_20.5+0.5log_20.5) = 1.0$$
也就是对于投硬币这件事，产生的所有可能值包含的信息期望值为$1bit$。如果我们投两个硬币的话，也可以轻松得到这件事产生的所有可能值包含的信息期望值为$２bit$。假如，我们对硬币做一些手脚，使得投掷后正面朝上的概率为$0.8$，正面朝下的概率为$0.2$，那么它的熵为：$$H(X) = -(0.8log_20.8+0.2log_20.2) = 0.7219280948873623
$$可以看到，它的值比$1$小，虽然硬币投出仍然会产生两种结果，但是它的不确定小了。</p>
            </div>
            <!-- /.entry-content -->
        </article>
    </section>

        </div>
    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">&copy; 2016 Yin
            &middot; Powered by <a href="https://github.com/getpelican/pelican-themes/tree/master/pelican-bootstrap3" target="_blank">pelican-bootstrap3</a>,
            <a href="http://docs.getpelican.com/" target="_blank">Pelican</a>,
            <a href="http://getbootstrap.com" target="_blank">Bootstrap</a>         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="https://IMYin.github.io/theme/js/jquery.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="https://IMYin.github.io/theme/js/bootstrap.min.js"></script>

<!-- Enable responsive features in IE8 with Respond.js (https://github.com/scottjehl/Respond) -->
<script src="https://IMYin.github.io/theme/js/respond.min.js"></script>


</body>
</html>